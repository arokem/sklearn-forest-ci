{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For now, we're just pulling this out of a hat:\n",
    "\n",
    "var_hat = np.random.rand(100)\n",
    "var_hat_ss = np.random.rand(100)\n",
    "\n",
    "B = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calibration_ratio = 2\n",
    "n_sample = np.ceil(B / calibration_ratio)\n",
    "# Use this second set of variance estimates to estimate scale of Monte Carlo noise\n",
    "sigma2_ss = np.mean((var_hat_ss - var_hat)**2)\n",
    "delta = n_sample / B\n",
    "sigma2 = (delta**2 + (1 - delta)**2) / (2 * (1 - delta)**2) * sigma2_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19398131112751685"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neg_loglik(eta, XX):\n",
    "        g_eta_raw = np.exp(np.dot(XX, eta)) * float(xvals >= 0)\n",
    "        if (\n",
    "         (sum(g_eta_raw) == math.inf) |\n",
    "         (sum(g_eta_raw) <= 100 * np.finfo(np.double).tiny)):\n",
    "            return (1000 * (length(X) + sum(eta ** 2)))\n",
    "\n",
    "        g_eta_main = g_eta_raw / sum(g_eta_raw)\n",
    "        g_eta = ((1 - unif_fraction) * g_eta_main +\n",
    "                 unif_fraction * float(xvals >= 0) / sum(xvals >= 0))\n",
    "\n",
    "        f_eta = np.convolve(g_eta, noise_rotate)\n",
    "\n",
    "        return np.sum(np.interp(xvals,\n",
    "                      -np.log(maximum(f_eta, 0.0000001)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = var_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XX = sapply(1:p, function(j) xvals^j * as.numeric(xvals >= 0))\n",
    "min_x = min(min(X) - 2 * np.std(X), 0)\n",
    "max_x = max(max(X) + 2 * np.std(X))\n",
    "binw = (max_x - min_x) / (nbin - 1)\n",
    "xvals = np.arange(min_x, max_x + 1, binw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_dist(xvals, j):\n",
    "    (xvals ** j) * (xvals >= 0).astype(float)\n",
    "\n",
    "\n",
    "XX = np.zeros((p, len(xvals)), dtype=np.float)\n",
    "for ind, exp in enumerate(range(p)):\n",
    "    mask = np.ones_like(xvals)\n",
    "    mask[np.where(xvals <= 0)[0]] = 0\n",
    "    XX[ind, :] = np.pow(xvals, exp) * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gfit(X, sigma, p=2, nbin=1000, unif_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Fit an empirical Bayes prior in the hierarchical model\n",
    "        mu ~ G, X ~ N(mu, sigma^2)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ndarray\n",
    "        A 1D array of observations\n",
    "    sigma: float\n",
    "        noise estimate on X\n",
    "    p: int\n",
    "        tuning parameter -- number of parameters used to fit G\n",
    "    nbin: int\n",
    "        tuning parameter -- number of bins used for discrete approximation\n",
    "    unif_fraction: float\n",
    "        tuning parameter -- fraction of G modeled as \"slab\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An array of the posterior density estimate g\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    .. [Efron2014] B Efron. \"Two modeling strategies for empirical Bayes\n",
    "        estimation.\" Stat. Sci., 29(2): 285â€“301, 2014.\n",
    "    \"\"\"\n",
    "    min_x = min(min(X) - 2 * np.std(X), 0)\n",
    "    max_x = max(max(X) + 2 * np.std(X))\n",
    "    binw = (max_x - min_x) / (nbin - 1)\n",
    "    xvals = np.arange(min_x, max_x + 1, binw)\n",
    "\n",
    "    zero_idx = max(np.where(xvals <= 0)[0])\n",
    "    noise_kernel = norm().pdf(xvals / sigma) * binw / sigma\n",
    "\n",
    "    if zero_idx > 0:\n",
    "        noise_rotate = noise_kernel[list(np.arange(zero_idx, len(xvals))) +\n",
    "                                    list(np.arange(0, zero_idx))]\n",
    "    else:\n",
    "        noise_rotate = noise_kernel\n",
    "\n",
    "    XX = np.zeros((p, len(xvals)), dtype=np.float)\n",
    "    for ind, exp in enumerate(range(p)):\n",
    "        mask = np.ones_like(xvals)\n",
    "        mask[np.where(xvals <= 0)[0]] = 0\n",
    "        XX[ind, :] = np.pow(xvals, exp) * mask\n",
    "\n",
    "    eta_hat = nlm(neg_loglik, rep(-1, p))$estimate\n",
    "    g_eta_raw = exp(np.dot(XX, eta_hat)) * float(xvals >= 0)\n",
    "    g_eta_main = g_eta_raw / sum(g_eta_raw)\n",
    "    g_eta = (1 - unif_fraction) * g_eta_main +\n",
    "    unif_fraction * float(xvals >= 0) / sum(xvals >= 0)\n",
    "\n",
    "    return xvals, g_eta"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
